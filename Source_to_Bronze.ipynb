{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "21t9kS_KeY2g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify the contents of the landregistry-public-data bucket\n",
        "import boto3\n",
        "from google.colab import userdata\n",
        "\n",
        "AWS_KEY   = userdata.get('AWS_ACCESS_KEY_ID')\n",
        "AWS_SEC   = userdata.get('AWS_SECRET_ACCESS_KEY')\n",
        "\n",
        "src_s3 = boto3.client('s3',\n",
        "                      aws_access_key_id=AWS_KEY,\n",
        "                      aws_secret_access_key=AWS_SEC)\n",
        "\n",
        "bucket_name = 'uk-property-bronze'\n",
        "\n",
        "try:\n",
        "    response = src_s3.list_objects_v2(Bucket=bucket_name, RequestPayer='requester')\n",
        "    if 'Contents' in response:\n",
        "        print(f\"Objects in bucket '{bucket_name}':\")\n",
        "        for obj in response['Contents']:\n",
        "            print(obj['Key'])\n",
        "    else:\n",
        "        print(f\"No objects found in bucket '{bucket_name}'.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FT0secxVfgIW",
        "outputId": "d69140b6-711a-4524-a4d9-b8ef3603d661"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Objects in bucket 'uk-property-bronze':\n",
            "raw/ppd/2025-09-15T074310Z_ea8fac7c65fb589b0d53560f5251f74f9e9b243478dcb6b3ea79b5e36449c8d9.csv\n",
            "raw/ppd/2025-09-15T074725Z_ea8fac7c65fb589b0d53560f5251f74f9e9b243478dcb6b3ea79b5e36449c8d9.csv\n",
            "raw/ppd/2025-09-15T074857Z_ea8fac7c65fb589b0d53560f5251f74f9e9b243478dcb6b3ea79b5e36449c8d9.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o5vDr1l7hQjl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HlyNZT7CjPmp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QRXtHBaJkh8z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jqyADTZMkh6E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === 1. installs ===\n",
        "!pip -q install boto3 smart-open requests tqdm\n",
        "\n",
        "# === 2. AWS auth ===\n",
        "import os\n",
        "from google.colab import userdata   # Colab secret manager\n",
        "os.environ[\"AWS_ACCESS_KEY_ID\"]     = userdata.get('AWS_ACCESS_KEY_ID')\n",
        "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = userdata.get('AWS_SECRET_ACCESS_KEY')\n",
        "os.environ[\"AWS_DEFAULT_REGION\"]    = \"eu-west-1\"   # or your bucket region\n",
        "\n",
        "import boto3, requests, smart_open, datetime, json\n",
        "from tqdm import tqdm\n",
        "\n",
        "s3 = boto3.client('s3')\n",
        "BUCKET = \"uk-property-bronze\"   # <── change\n",
        "S3_KEY = \"raw/hmlr/price-paid/price-paid-latest.csv\"\n",
        "SUCCESS_KEY = \"raw/hmlr/price-paid/_SUCCESS\""
      ],
      "metadata": {
        "id": "wTfWtGBfkh3E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hUQdM2LguXT6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0_HcElgguXRJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install boto3 smart-open tqdm"
      ],
      "metadata": {
        "id": "XeA9W6BFlHeE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#HMLR txt BRONZE"
      ],
      "metadata": {
        "id": "JQl6CsEu3if_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, boto3, requests, concurrent.futures, datetime, json\n",
        "from google.colab import userdata\n",
        "os.environ[\"AWS_ACCESS_KEY_ID\"]     = userdata.get('AWS_ACCESS_KEY_ID')\n",
        "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = userdata.get('AWS_SECRET_ACCESS_KEY')\n",
        "os.environ[\"AWS_DEFAULT_REGION\"]    = \"eu-west-1\"\n",
        "\n",
        "BUCKET = \"uk-property-bronze\"\n",
        "BASE_S3_KEY = \"bronze/hmlr/price-paid/yearly_txt/\"\n",
        "\n",
        "urls = {\n",
        "    \"2018\": \"http://prod2.publicdata.landregistry.gov.uk.s3-website-eu-west-1.amazonaws.com/pp-2018.txt\",\n",
        "    \"2019\": \"http://prod.publicdata.landregistry.gov.uk.s3-website-eu-west-1.amazonaws.com/pp-2019.txt\",\n",
        "    \"2020\": \"http://prod.publicdata.landregistry.gov.uk.s3-website-eu-west-1.amazonaws.com/pp-2020.txt\",\n",
        "    \"2021\": \"http://prod.publicdata.landregistry.gov.uk.s3-website-eu-west-1.amazonaws.com/pp-2021.txt\",\n",
        "    \"2022\": \"http://prod.publicdata.landregistry.gov.uk.s3-website-eu-west-1.amazonaws.com/pp-2022.txt\",\n",
        "    \"2023\": \"http://prod.publicdata.landregistry.gov.uk.s3-website-eu-west-1.amazonaws.com/pp-2023.txt\",\n",
        "    \"2024\": \"http://prod.publicdata.landregistry.gov.uk.s3-website-eu-west-1.amazonaws.com/pp-2024.txt\",\n",
        "    \"2025\": \"http://prod.publicdata.landregistry.gov.uk.s3-website-eu-west-1.amazonaws.com/pp-2025.txt\",\n",
        "}\n",
        "\n",
        "s3 = boto3.client('s3')\n",
        "results = []\n",
        "\n",
        "def upload_one(year_url):\n",
        "    year, url = year_url\n",
        "    fname = f\"pp-{year}.txt\"\n",
        "    s3_key  = BASE_S3_KEY + fname\n",
        "    with requests.get(url, stream=True, timeout=60) as r:\n",
        "        r.raise_for_status()\n",
        "        with smart_open.open(f\"s3://{BUCKET}/{s3_key}\", 'wb',\n",
        "                             transport_params={'client': s3}) as s3_file:\n",
        "            for chunk in r.iter_content(chunk_size=8*1024*1024):\n",
        "                if chunk:\n",
        "                    s3_file.write(chunk)\n",
        "    results.append({\"year\": year, \"s3_key\": s3_key, \"url\": url})\n",
        "    return year\n",
        "\n",
        "# ===== 3. parallel pull → S3 =====\n",
        "from tqdm import tqdm\n",
        "with concurrent.futures.ThreadPoolExecutor(max_workers=8) as ex:\n",
        "    list(tqdm(ex.map(upload_one, urls.items()), total=len(urls)))\n",
        "\n",
        "# ===== 4. success flag =====\n",
        "s3.put_object(Bucket=BUCKET, Key=BASE_S3_KEY+\"_SUCCESS\", Body=b'')\n",
        "\n",
        "# ===== 5. log for Git =====\n",
        "log = {\n",
        "    \"source\": \"hmlr_yearly_txt\",\n",
        "    \"ingest_ts\": datetime.datetime.utcnow().isoformat(),\n",
        "    \"bucket\": BUCKET,\n",
        "    \"objects\": results\n",
        "}\n",
        "print(json.dumps(log, indent=2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qym5CQeNuYGx",
        "outputId": "0a9fc101-b590-4c7b-bba5-ffce04c41fb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 8/8 [00:20<00:00,  2.62s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"source\": \"hmlr_yearly_txt\",\n",
            "  \"ingest_ts\": \"2025-09-15T09:47:51.867004\",\n",
            "  \"bucket\": \"uk-property-bronze\",\n",
            "  \"objects\": [\n",
            "    {\n",
            "      \"year\": \"2025\",\n",
            "      \"s3_key\": \"bronze/hmlr/price-paid/yearly_txt/pp-2025.txt\",\n",
            "      \"url\": \"http://prod.publicdata.landregistry.gov.uk.s3-website-eu-west-1.amazonaws.com/pp-2025.txt\"\n",
            "    },\n",
            "    {\n",
            "      \"year\": \"2023\",\n",
            "      \"s3_key\": \"bronze/hmlr/price-paid/yearly_txt/pp-2023.txt\",\n",
            "      \"url\": \"http://prod.publicdata.landregistry.gov.uk.s3-website-eu-west-1.amazonaws.com/pp-2023.txt\"\n",
            "    },\n",
            "    {\n",
            "      \"year\": \"2024\",\n",
            "      \"s3_key\": \"bronze/hmlr/price-paid/yearly_txt/pp-2024.txt\",\n",
            "      \"url\": \"http://prod.publicdata.landregistry.gov.uk.s3-website-eu-west-1.amazonaws.com/pp-2024.txt\"\n",
            "    },\n",
            "    {\n",
            "      \"year\": \"2020\",\n",
            "      \"s3_key\": \"bronze/hmlr/price-paid/yearly_txt/pp-2020.txt\",\n",
            "      \"url\": \"http://prod.publicdata.landregistry.gov.uk.s3-website-eu-west-1.amazonaws.com/pp-2020.txt\"\n",
            "    },\n",
            "    {\n",
            "      \"year\": \"2019\",\n",
            "      \"s3_key\": \"bronze/hmlr/price-paid/yearly_txt/pp-2019.txt\",\n",
            "      \"url\": \"http://prod.publicdata.landregistry.gov.uk.s3-website-eu-west-1.amazonaws.com/pp-2019.txt\"\n",
            "    },\n",
            "    {\n",
            "      \"year\": \"2022\",\n",
            "      \"s3_key\": \"bronze/hmlr/price-paid/yearly_txt/pp-2022.txt\",\n",
            "      \"url\": \"http://prod.publicdata.landregistry.gov.uk.s3-website-eu-west-1.amazonaws.com/pp-2022.txt\"\n",
            "    },\n",
            "    {\n",
            "      \"year\": \"2018\",\n",
            "      \"s3_key\": \"bronze/hmlr/price-paid/yearly_txt/pp-2018.txt\",\n",
            "      \"url\": \"http://prod2.publicdata.landregistry.gov.uk.s3-website-eu-west-1.amazonaws.com/pp-2018.txt\"\n",
            "    },\n",
            "    {\n",
            "      \"year\": \"2021\",\n",
            "      \"s3_key\": \"bronze/hmlr/price-paid/yearly_txt/pp-2021.txt\",\n",
            "      \"url\": \"http://prod.publicdata.landregistry.gov.uk.s3-website-eu-west-1.amazonaws.com/pp-2021.txt\"\n",
            "    }\n",
            "  ]\n",
            "}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "/tmp/ipython-input-3985684947.py:49: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  \"ingest_ts\": datetime.datetime.utcnow().isoformat(),\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#HMLR parkeet BRONZE"
      ],
      "metadata": {
        "id": "RiUzgI7o3nDD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, boto3, requests, concurrent.futures, datetime, json, pandas as pd, pyarrow as pa, pyarrow.parquet as pq\n",
        "from google.colab import userdata\n",
        "os.environ[\"AWS_ACCESS_KEY_ID\"]     = userdata.get('AWS_ACCESS_KEY_ID')\n",
        "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = userdata.get('AWS_SECRET_ACCESS_KEY')\n",
        "os.environ[\"AWS_DEFAULT_REGION\"]    = \"eu-west-1\"\n",
        "\n",
        "BUCKET = \"uk-property-bronze\"\n",
        "BASE_S3_KEY = \"bronze/hmlr/price-paid/yearly_parquet/\"\n",
        "\n",
        "urls = {   # same dict you already have\n",
        "    \"2018\": \"http://prod2.publicdata.landregistry.gov.uk.s3-website-eu-west-1.amazonaws.com/pp-2018.txt\",\n",
        "    \"2019\": \"http://prod.publicdata.landregistry.gov.uk.s3-website-eu-west-1.amazonaws.com/pp-2019.txt\",\n",
        "    \"2020\": \"http://prod.publicdata.landregistry.gov.uk.s3-website-eu-west-1.amazonaws.com/pp-2020.txt\",\n",
        "    \"2021\": \"http://prod.publicdata.landregistry.gov.uk.s3-website-eu-west-1.amazonaws.com/pp-2021.txt\",\n",
        "    \"2022\": \"http://prod.publicdata.landregistry.gov.uk.s3-website-eu-west-1.amazonaws.com/pp-2022.txt\",\n",
        "    \"2023\": \"http://prod.publicdata.landregistry.gov.uk.s3-website-eu-west-1.amazonaws.com/pp-2023.txt\",\n",
        "    \"2024\": \"http://prod.publicdata.landregistry.gov.uk.s3-website-eu-west-1.amazonaws.com/pp-2024.txt\",\n",
        "    \"2025\": \"http://prod.publicdata.landregistry.gov.uk.s3-website-eu-west-1.amazonaws.com/pp-2025.txt\",\n",
        "}\n",
        "\n",
        "s3 = boto3.client('s3')\n",
        "results = []\n",
        "\n",
        "def txt_to_parquet(year_url):\n",
        "    year, url = year_url\n",
        "    # 1. stream txt into mem (files are <250 MB each)\n",
        "    with requests.get(url, timeout=60) as r:\n",
        "        r.raise_for_status()\n",
        "        df = pd.read_csv(io.BytesIO(r.content), header=None, low_memory=False)\n",
        "\n",
        "    # 2. standard HMLR txt schema (no header in file)\n",
        "    cols = [\"transaction_id\",\"price\",\"date\",\"postcode\",\"prop_type\",\"old_new\",\"duration\",\"paon\",\"saon\",\n",
        "            \"street\",\"locality\",\"town\",\"district\",\"county\",\"ppd_cat\",\"status\"]\n",
        "    df.columns = cols\n",
        "    df[\"year\"] = int(year)          # partition column\n",
        "\n",
        "    # 3. write partitioned parquet straight to S3\n",
        "    parquet_key = f\"{BASE_S3_KEY}year={year}/part-0.parquet\"\n",
        "    with smart_open.open(f\"s3://{BUCKET}/{parquet_key}\", 'wb',\n",
        "                         transport_params={'client': s3}) as s3_file:\n",
        "        table = pa.Table.from_pandas(df)\n",
        "        pq.write_table(table, s3_file, compression='snappy')\n",
        "\n",
        "    results.append({\"year\": year, \"parquet_key\": parquet_key, \"rows\": len(df)})\n",
        "    return year\n",
        "\n",
        "# ===== 2. parallel convert → parquet =====\n",
        "import io\n",
        "from tqdm import tqdm\n",
        "with concurrent.futures.ThreadPoolExecutor(max_workers=8) as ex:\n",
        "    list(tqdm(ex.map(txt_to_parquet, urls.items()), total=len(urls)))\n",
        "\n",
        "# ===== 3. success flag =====\n",
        "s3.put_object(Bucket=BUCKET, Key=BASE_S3_KEY+\"_SUCCESS\", Body=b'')\n",
        "\n",
        "# ===== 4. log =====\n",
        "log = {\n",
        "    \"source\": \"hmlr_yearly_parquet\",\n",
        "    \"ingest_ts\": datetime.datetime.utcnow().isoformat(),\n",
        "    \"bucket\": BUCKET,\n",
        "    \"objects\": results\n",
        "}\n",
        "print(json.dumps(log, indent=2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B25CK-aVufZg",
        "outputId": "e85a9b21-a263-483a-b167-3792e76154e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 8/8 [01:10<00:00,  8.85s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"source\": \"hmlr_yearly_parquet\",\n",
            "  \"ingest_ts\": \"2025-09-15T09:49:15.304940\",\n",
            "  \"bucket\": \"uk-property-bronze\",\n",
            "  \"objects\": [\n",
            "    {\n",
            "      \"year\": \"2025\",\n",
            "      \"parquet_key\": \"bronze/hmlr/price-paid/yearly_parquet/year=2025/part-0.parquet\",\n",
            "      \"rows\": 314860\n",
            "    },\n",
            "    {\n",
            "      \"year\": \"2023\",\n",
            "      \"parquet_key\": \"bronze/hmlr/price-paid/yearly_parquet/year=2023/part-0.parquet\",\n",
            "      \"rows\": 853480\n",
            "    },\n",
            "    {\n",
            "      \"year\": \"2024\",\n",
            "      \"parquet_key\": \"bronze/hmlr/price-paid/yearly_parquet/year=2024/part-0.parquet\",\n",
            "      \"rows\": 841227\n",
            "    },\n",
            "    {\n",
            "      \"year\": \"2020\",\n",
            "      \"parquet_key\": \"bronze/hmlr/price-paid/yearly_parquet/year=2020/part-0.parquet\",\n",
            "      \"rows\": 896182\n",
            "    },\n",
            "    {\n",
            "      \"year\": \"2019\",\n",
            "      \"parquet_key\": \"bronze/hmlr/price-paid/yearly_parquet/year=2019/part-0.parquet\",\n",
            "      \"rows\": 1011611\n",
            "    },\n",
            "    {\n",
            "      \"year\": \"2022\",\n",
            "      \"parquet_key\": \"bronze/hmlr/price-paid/yearly_parquet/year=2022/part-0.parquet\",\n",
            "      \"rows\": 1072476\n",
            "    },\n",
            "    {\n",
            "      \"year\": \"2018\",\n",
            "      \"parquet_key\": \"bronze/hmlr/price-paid/yearly_parquet/year=2018/part-0.parquet\",\n",
            "      \"rows\": 1037301\n",
            "    },\n",
            "    {\n",
            "      \"year\": \"2021\",\n",
            "      \"parquet_key\": \"bronze/hmlr/price-paid/yearly_parquet/year=2021/part-0.parquet\",\n",
            "      \"rows\": 1279065\n",
            "    }\n",
            "  ]\n",
            "}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "/tmp/ipython-input-3707376786.py:59: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  \"ingest_ts\": datetime.datetime.utcnow().isoformat(),\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Metadata of HMLR"
      ],
      "metadata": {
        "id": "HUjbnmXs0ehI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the region of the S3 bucket\n",
        "import boto3\n",
        "from google.colab import userdata\n",
        "\n",
        "AWS_KEY   = userdata.get('AWS_ACCESS_KEY_ID')\n",
        "AWS_SEC   = userdata.get('AWS_SECRET_ACCESS_KEY')\n",
        "BUCKET = \"uk-property-bronze\"\n",
        "\n",
        "try:\n",
        "    s3_client = boto3.client('s3',\n",
        "                             aws_access_key_id=AWS_KEY,\n",
        "                             aws_secret_access_key=AWS_SEC)\n",
        "    response = s3_client.get_bucket_location(Bucket=BUCKET)\n",
        "    region = response.get('LocationConstraint')\n",
        "    if region is None:\n",
        "        # Buckets in us-east-1 have a None location constraint\n",
        "        region = 'us-east-1'\n",
        "    print(f\"Region of bucket '{BUCKET}': {region}\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pIWVT8i90iL5",
        "outputId": "880eca54-731f-47a1-c5df-56b57c97ca68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Region of bucket 'uk-property-bronze': eu-north-1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== 0. deps =====\n",
        "!pip -q install boto3 smart-open duckdb pyarrow pandas tqdm\n",
        "\n",
        "# ===== 1. auth (same as always) =====\n",
        "import os, boto3, json, datetime, duckdb, pyarrow.parquet as pq\n",
        "from smart_open import open as sm_open\n",
        "from tqdm import tqdm\n",
        "from google.colab import userdata\n",
        "os.environ[\"AWS_ACCESS_KEY_ID\"]     = userdata.get('AWS_ACCESS_KEY_ID')\n",
        "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = userdata.get('AWS_SECRET_ACCESS_KEY')\n",
        "os.environ[\"AWS_DEFAULT_REGION\"]    = \"eu-west-1\"\n",
        "\n",
        "BUCKET   = \"uk-property-bronze\"\n",
        "PARQUET_PREFIX = \"bronze/hmlr/price-paid/yearly_parquet/\"\n",
        "META_KEY       = \"bronze/hmlr/price-paid/yearly_parquet_metadata/\" + datetime.date.today().isoformat() + \"_hmlr_metadata.json\"\n",
        "\n",
        "s3 = boto3.client('s3')\n",
        "\n",
        "# ===== 2. list parquet objects =====\n",
        "paginator = s3.get_paginator('list_objects_v2')\n",
        "pages = paginator.paginate(Bucket=BUCKET, Prefix=PARQUET_PREFIX)\n",
        "keys = [obj['Key'] for page in pages for obj in page.get('Contents', [])\n",
        "        if obj['Key'].endswith('.parquet')]\n",
        "assert keys, \"No parquet files found – check prefix\"\n",
        "\n",
        "# ===== 3. build metadata per file =====\n",
        "import boto3.session, duckdb\n",
        "session = boto3.session.Session()          # picks up env region\n",
        "credentials = session.get_credentials()\n",
        "region = session.region_name or 'eu-west-1'\n",
        "\n",
        "meta_list = []\n",
        "for key in tqdm(keys):\n",
        "    # 1. basic file stats\n",
        "    head = s3.head_object(Bucket=BUCKET, Key=key)\n",
        "    size = head['ContentLength']\n",
        "\n",
        "    # 2. parquet footer only (no DuckDB)\n",
        "    with sm_open(f\"s3://{BUCKET}/{key}\", 'rb', transport_params={'client': s3}) as f:\n",
        "        pf = pq.ParquetFile(f)\n",
        "        meta      = pf.metadata\n",
        "        rows      = meta.num_rows\n",
        "        cols      = meta.num_columns\n",
        "        # human-readable schema\n",
        "        schema_str = str({meta.schema.column(i).name: str(meta.schema.column(i).physical_type)\n",
        "                          for i in range(cols)})\n",
        "\n",
        "    # 3. lightweight column stats (min/max) – footer only\n",
        "    stats = []\n",
        "    for i in range(cols):\n",
        "        col_meta = meta.row_group(0).column(i)\n",
        "        stats.append({\n",
        "            \"name\": meta.schema.column(i).name,\n",
        "            \"min\": col_meta.statistics.min,\n",
        "            \"max\": col_meta.statistics.max,\n",
        "            \"nulls\": col_meta.statistics.null_count\n",
        "        })\n",
        "\n",
        "    year = key.split(\"year=\")[1].split(\"/\")[0]\n",
        "    meta_list.append({\n",
        "        \"year\": year,\n",
        "        \"s3_key\": key,\n",
        "        \"size_bytes\": size,\n",
        "        \"rows\": rows,\n",
        "        \"columns\": cols,\n",
        "        \"schema\": schema_str,\n",
        "        \"column_stats\": stats\n",
        "    })\n",
        "# ===== 4. consolidate & upload ===== (same as before)\n",
        "meta_json = json.dumps({\n",
        "    \"source\": \"hmlr_yearly_parquet_metadata\",\n",
        "    \"ingest_ts\": datetime.datetime.utcnow().isoformat(),\n",
        "    \"bucket\": BUCKET,\n",
        "    \"metadata\": meta_list\n",
        "}, indent=2)\n",
        "\n",
        "with sm_open(f\"s3://{BUCKET}/{META_KEY}\", 'w',\n",
        "             transport_params={'client': s3}) as f:\n",
        "    f.write(meta_json)\n",
        "\n",
        "print(f\"Metadata written to s3://{BUCKET}/{META_KEY}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ifU9b6ZNv44v",
        "outputId": "e1bee489-db6f-49ee-fdb1-f2836beb57de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 8/8 [00:04<00:00,  1.67it/s]\n",
            "/tmp/ipython-input-2162340293.py:72: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  \"ingest_ts\": datetime.datetime.utcnow().isoformat(),\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metadata written to s3://uk-property-bronze/bronze/hmlr/price-paid/yearly_parquet_metadata/2025-09-15_hmlr_metadata.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SEED_URLS = [\n",
        "    \"https://www.rightmove.co.uk/news/\",\n",
        "    \"https://www.zoopla.co.uk/discover/property-news/\",\n",
        "    \"https://www.bbc.co.uk/news/topics/cp7r8yvllvdt\",  # BBC house prices\n",
        "    \"https://www.theguardian.com/money/house-prices\",\n",
        "    \"https://www.hometrack.com/uk-insight/\",            # city-level reports\n",
        "]"
      ],
      "metadata": {
        "id": "d6l0Iwl-04mn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== 0. install =====\n",
        "!pip -q install firecrawl-py boto3 smart-open tqdm\n",
        "\n",
        "# ===== 1. auth =====\n",
        "import os, json, datetime, hashlib, boto3\n",
        "from smart_open import open as sm_open\n",
        "from firecrawl import FirecrawlApp\n",
        "from google.colab import userdata\n",
        "from tqdm import tqdm\n",
        "\n",
        "FIRECRAWL_KEY = userdata.get('FIRECRAWL_API_KEY')\n",
        "AWS_KEY       = userdata.get('AWS_ACCESS_KEY_ID')\n",
        "AWS_SECRET    = userdata.get('AWS_SECRET_ACCESS_KEY')\n",
        "REGION        = \"eu-west-1\"\n",
        "BUCKET        = \"uk-property-bronze\"          # ← yours\n",
        "PREFIX        = \"bronze/firecrawl/uk-local-news/\"\n",
        "\n",
        "os.environ.update({\n",
        "    \"AWS_ACCESS_KEY_ID\": AWS_KEY,\n",
        "    \"AWS_SECRET_ACCESS_KEY\": AWS_SECRET,\n",
        "    \"AWS_DEFAULT_REGION\": REGION\n",
        "})\n",
        "\n",
        "s3  = boto3.client('s3')\n",
        "app = FirecrawlApp(api_key=FIRECRAWL_KEY)\n",
        "\n",
        "# ===== 2. only Rightmove + Zoopla =====\n",
        "SEED_URLS = [\n",
        "    \"https://www.rightmove.co.uk/news/\",\n",
        "    \"https://www.zoopla.co.uk/discover/property-news/\"\n",
        "]\n",
        "\n",
        "# ===== 3. crawl → bronze =====\n",
        "results = []\n",
        "for url in tqdm(SEED_URLS):\n",
        "    print(\"crawling →\", url)\n",
        "    try:\n",
        "        out = app.scrape(\n",
        "            url,\n",
        "            page_options={\"includeHtml\": False, \"includeMarkdown\": True}\n",
        "        )\n",
        "        md = out.get(\"markdown\", \"\").strip()\n",
        "        if not md:\n",
        "            print(\"⚠️  empty markdown for\", url)\n",
        "            continue\n",
        "    except Exception as e:\n",
        "        print(\"❌ skip\", url, e)\n",
        "        continue\n",
        "\n",
        "    slug = hashlib.md5(url.encode()).hexdigest()[:8]\n",
        "    key  = f\"{PREFIX}{datetime.datetime.now(datetime.timezone.utc).isoformat()[:10]}/{slug}.json\"\n",
        "    payload = {\n",
        "        \"url\": url,\n",
        "        \"crawl_ts\": datetime.datetime.now(datetime.timezone.utc).isoformat(),\n",
        "        \"markdown\": md,\n",
        "        \"metadata\": out.get(\"metadata\", {})\n",
        "    }\n",
        "\n",
        "    with sm_open(f\"s3://{BUCKET}/{key}\", 'w',\n",
        "                 transport_params={'client': s3}) as f:\n",
        "        f.write(json.dumps(payload, ensure_ascii=False))\n",
        "\n",
        "    results.append({\"url\": url, \"s3_key\": key, \"len_md\": len(md)})\n",
        "    print(\"✅ written\", key, f\"({len(md)} chars)\")\n",
        "\n",
        "# ===== 4. success flag + log =====\n",
        "s3.put_object(Bucket=BUCKET, Key=PREFIX+\"_SUCCESS\", Body=b'')\n",
        "log = {\n",
        "    \"source\": \"firecrawl_rm_zoopla\",\n",
        "    \"ingest_ts\": datetime.datetime.now(datetime.timezone.utc).isoformat(),\n",
        "    \"bucket\": BUCKET,\n",
        "    \"crawls\": results\n",
        "}\n",
        "print(json.dumps(log, indent=2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ah5iaBja9adZ",
        "outputId": "e0445e0e-cb37-4d2d-f699-1f3f886c7d93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2/2 [00:00<00:00, 11081.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "crawling → https://www.rightmove.co.uk/news/\n",
            "❌ skip https://www.rightmove.co.uk/news/ FirecrawlClient.scrape() got an unexpected keyword argument 'page_options'\n",
            "crawling → https://www.zoopla.co.uk/discover/property-news/\n",
            "❌ skip https://www.zoopla.co.uk/discover/property-news/ FirecrawlClient.scrape() got an unexpected keyword argument 'page_options'\n",
            "{\n",
            "  \"source\": \"firecrawl_rm_zoopla\",\n",
            "  \"ingest_ts\": \"2025-09-15T10:48:39.614471+00:00\",\n",
            "  \"bucket\": \"uk-property-bronze\",\n",
            "  \"crawls\": []\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#FIRECRAWLLER (Zoopla, RightMove)"
      ],
      "metadata": {
        "id": "dbWQfTAmDVaU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from firecrawl import FirecrawlApp\n",
        "print(dir(FirecrawlApp))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CrJUF4-XAdeh",
        "outputId": "a83a7da0-9694-4007-df5c-89fd89e0e545"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from firecrawl import FirecrawlApp\n",
        "import inspect\n",
        "app = FirecrawlApp(api_key=\"dummy\")   # key not used for inspect\n",
        "print([m for m in dir(app) if not m.startswith('_')])\n",
        "print(inspect.signature(app.scrape))   # or whatever method looks right"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-kdKdcIGAsLw",
        "outputId": "76514e45-e20d-4641-a13e-054ee5b1126b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['active_crawls', 'api_key', 'api_url', 'batch_scrape', 'cancel_batch_scrape', 'cancel_crawl', 'crawl', 'crawl_params_preview', 'extract', 'get_active_crawls', 'get_batch_scrape_errors', 'get_batch_scrape_status', 'get_concurrency', 'get_crawl_errors', 'get_crawl_status', 'get_credit_usage', 'get_extract_status', 'get_queue_status', 'get_token_usage', 'map', 'scrape', 'search', 'start_batch_scrape', 'start_crawl', 'start_extract', 'v1', 'v2', 'watcher']\n",
            "(url: str, *, formats: Optional[List[ForwardRef('FormatOption')]] = None, headers: Optional[Dict[str, str]] = None, include_tags: Optional[List[str]] = None, exclude_tags: Optional[List[str]] = None, only_main_content: Optional[bool] = None, timeout: Optional[int] = None, wait_for: Optional[int] = None, mobile: Optional[bool] = None, parsers: Union[List[str], List[Union[str, firecrawl.v2.types.PDFParser]], NoneType] = None, actions: Optional[List[Union[ForwardRef('WaitAction'), ForwardRef('ScreenshotAction'), ForwardRef('ClickAction'), ForwardRef('WriteAction'), ForwardRef('PressAction'), ForwardRef('ScrollAction'), ForwardRef('ScrapeAction'), ForwardRef('ExecuteJavascriptAction'), ForwardRef('PDFAction')]]] = None, location: Optional[ForwardRef('Location')] = None, skip_tls_verification: Optional[bool] = None, remove_base64_images: Optional[bool] = None, fast_mode: Optional[bool] = None, use_mock: Optional[str] = None, block_ads: Optional[bool] = None, proxy: Optional[str] = None, max_age: Optional[int] = None, store_in_cache: Optional[bool] = None, integration: Optional[str] = None) -> firecrawl.v2.types.Document\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Fire Crawlling Rightmove"
      ],
      "metadata": {
        "id": "3XNQN5ryIAGV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== minimal firecrawl debug + write =====\n",
        "!pip -q install firecrawl-py boto3 smart-open\n",
        "\n",
        "import os, json, datetime, hashlib, boto3\n",
        "from smart_open import open as sm_open\n",
        "from firecrawl import FirecrawlApp\n",
        "from google.colab import userdata\n",
        "import datetime as dt\n",
        "\n",
        "# auth\n",
        "os.environ.update({\n",
        "    \"AWS_ACCESS_KEY_ID\": userdata.get('AWS_ACCESS_KEY_ID'),\n",
        "    \"AWS_SECRET_ACCESS_KEY\": userdata.get('AWS_SECRET_ACCESS_KEY'),\n",
        "    \"AWS_DEFAULT_REGION\": \"eu-west-1\"\n",
        "})\n",
        "BUCKET = \"uk-property-bronze\"\n",
        "PREFIX = \"bronze/firecrawl/uk-local-news/\"\n",
        "\n",
        "s3  = boto3.client('s3')\n",
        "app = FirecrawlApp(api_key=userdata.get('FIRECRAWL_API_KEY'))\n",
        "\n",
        "URL = \"https://www.rightmove.co.uk/news/\"\n",
        "\n",
        "print(\"🔍 scraping ---\", URL)\n",
        "try:\n",
        "    out = app.scrape(URL, formats=[\"markdown\"])   # <-- exact signature\n",
        "except Exception as e:\n",
        "    raise RuntimeError(f\"Firecrawl failed: {e}\") from e\n",
        "\n",
        "md = out.markdown or \"\"\n",
        "print(\"📄 markdown length:\", len(md))\n",
        "if not md:\n",
        "    raise ValueError(\"Empty markdown returned\")\n",
        "\n",
        "# write to bronze\n",
        "key = f\"{PREFIX}{dt.datetime.now(dt.timezone.utc).isoformat()[:10]}/rightmove.json\"\n",
        "payload = {\n",
        "    \"url\": URL,\n",
        "    \"crawl_ts\": dt.datetime.now(dt.timezone.utc).isoformat(),\n",
        "    \"markdown\": md,\n",
        "    \"metadata\": out.metadata.dict() if out.metadata else {}  # ← serialisable\n",
        "}\n",
        "\n",
        "with sm_open(f\"s3://{BUCKET}/{key}\", 'w', transport_params={'client': s3}) as f:\n",
        "    f.write(json.dumps(payload, ensure_ascii=False, indent=2))\n",
        "\n",
        "s3.put_object(Bucket=BUCKET, Key=PREFIX+\"_SUCCESS\", Body=b'')\n",
        "print(\"✅ written to\", key)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aVh8aQQCA4GQ",
        "outputId": "01f9f433-29e8-4591-acef-2a1ab387d371"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔍 scraping --- https://www.rightmove.co.uk/news/\n",
            "📄 markdown length: 9048\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-923751762.py:41: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
            "  \"metadata\": out.metadata.dict() if out.metadata else {}  # ← serialisable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ written to bronze/firecrawl/uk-local-news/2025-09-15/rightmove.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Fire Crawlling Zoopla"
      ],
      "metadata": {
        "id": "z_j2i_1DIEhz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== minimal firecrawl debug + write =====\n",
        "!pip -q install firecrawl-py boto3 smart-open\n",
        "\n",
        "import os, json, datetime, hashlib, boto3\n",
        "from smart_open import open as sm_open\n",
        "from firecrawl import FirecrawlApp\n",
        "from google.colab import userdata\n",
        "import datetime as dt\n",
        "\n",
        "# auth\n",
        "os.environ.update({\n",
        "    \"AWS_ACCESS_KEY_ID\": userdata.get('AWS_ACCESS_KEY_ID'),\n",
        "    \"AWS_SECRET_ACCESS_KEY\": userdata.get('AWS_SECRET_ACCESS_KEY'),\n",
        "    \"AWS_DEFAULT_REGION\": \"eu-west-1\"\n",
        "})\n",
        "BUCKET = \"uk-property-bronze\"\n",
        "PREFIX = \"bronze/firecrawl/uk-local-news/\"\n",
        "\n",
        "s3  = boto3.client('s3')\n",
        "app = FirecrawlApp(api_key=userdata.get('FIRECRAWL_API_KEY'))\n",
        "\n",
        "URL = \"https://www.zoopla.co.uk/discover/property-news/\"\n",
        "\n",
        "print(\"🔍 scraping ---\", URL)\n",
        "try:\n",
        "    out = app.scrape(URL, formats=[\"markdown\"])   # <-- exact signature\n",
        "except Exception as e:\n",
        "    raise RuntimeError(f\"Firecrawl failed: {e}\") from e\n",
        "\n",
        "md = out.markdown or \"\"\n",
        "print(\"📄 markdown length:\", len(md))\n",
        "if not md:\n",
        "    raise ValueError(\"Empty markdown returned\")\n",
        "\n",
        "# write to bronze\n",
        "key = f\"{PREFIX}{dt.datetime.now(dt.timezone.utc).isoformat()[:10]}/rightmove.json\"\n",
        "payload = {\n",
        "    \"url\": URL,\n",
        "    \"crawl_ts\": dt.datetime.now(dt.timezone.utc).isoformat(),\n",
        "    \"markdown\": md,\n",
        "    \"metadata\": out.metadata.dict() if out.metadata else {}  # ← serialisable\n",
        "}\n",
        "\n",
        "with sm_open(f\"s3://{BUCKET}/{key}\", 'w', transport_params={'client': s3}) as f:\n",
        "    f.write(json.dumps(payload, ensure_ascii=False, indent=2))\n",
        "\n",
        "s3.put_object(Bucket=BUCKET, Key=PREFIX+\"_SUCCESS\", Body=b'')\n",
        "print(\"✅ written to\", key)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0zEfR2M6CVJO",
        "outputId": "6676caee-a209-4fa3-d33e-3a633f694d03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔍 scraping --- https://www.zoopla.co.uk/discover/property-news/\n",
            "📄 markdown length: 5252\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-323165061.py:41: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
            "  \"metadata\": out.metadata.dict() if out.metadata else {}  # ← serialisable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ written to bronze/firecrawl/uk-local-news/2025-09-15/rightmove.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZembeM4MAP9Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###EPC_Data to Bronze S3"
      ],
      "metadata": {
        "id": "6yXpI8TMIJDL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6GB ----> Bronze S3"
      ],
      "metadata": {
        "id": "ouBxQTa_IVGn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== 1. auth =====\n",
        "import os, boto3, requests, datetime, json\n",
        "from smart_open import open as sm_open\n",
        "from google.colab import userdata\n",
        "from tqdm import tqdm\n",
        "\n",
        "os.environ.update({\n",
        "    \"AWS_ACCESS_KEY_ID\": userdata.get('AWS_ACCESS_KEY_ID'),\n",
        "    \"AWS_SECRET_ACCESS_KEY\": userdata.get('AWS_SECRET_ACCESS_KEY'),\n",
        "    \"AWS_DEFAULT_REGION\": \"eu-west-1\"\n",
        "})\n",
        "BUCKET = \"uk-property-bronze\"               # ← yours\n",
        "S3_KEY = \"bronze/epc/domestic/epc-domestic-complete.csv\"\n",
        "SUCCESS_KEY = \"bronze/epc/domestic/_SUCCESS\"\n",
        "\n",
        "# ===== 2. EPC domestic direct link (always latest) =====\n",
        "\n",
        "EPC_URL = \"https://epc.opendatacommunities.org/api/v1/files/all-domestic-certificates.zip\"\n",
        "HEADERS = {\n",
        "    'Authorization': f'Basic {EPC_TOKEN}'   # no Accept needed – it’s a ZIP\n",
        "}\n",
        "S3_KEY  = \"bronze/epc/domestic/epc-domestic-all-certificates.zip\"\n",
        "\n",
        "\n",
        "\n",
        "# ===== 3. stream download → S3 =====\n",
        "resp = requests.get(EPC_URL, headers=HEADERS, stream=True, timeout=60)\n",
        "resp.raise_for_status()\n",
        "size = int(resp.headers.get('content-length', 0))\n",
        "\n",
        "with sm_open(f\"s3://{BUCKET}/{S3_KEY}\", 'wb',\n",
        "             transport_params={'client': boto3.client('s3')}) as s3_file:\n",
        "    for chunk in tqdm(resp.iter_content(chunk_size=8*1024*1024),\n",
        "                      total=size//(8*1024*1024), unit='MB'):\n",
        "        if chunk:\n",
        "            s3_file.write(chunk)\n",
        "\n",
        "# ===== 4. success flag =====\n",
        "boto3.client('s3').put_object(Bucket=BUCKET, Key=SUCCESS_KEY, Body=b'')\n",
        "\n",
        "# ===== 5. log for repo =====\n",
        "log = {\n",
        "    \"source\": \"epc_domestic_register\",\n",
        "    \"ingest_ts\": datetime.datetime.now(datetime.timezone.utc).isoformat(),\n",
        "    \"url\": EPC_URL,\n",
        "    \"s3_bucket\": BUCKET,\n",
        "    \"s3_key\": S3_KEY,\n",
        "    \"size_bytes\": size\n",
        "}\n",
        "print(json.dumps(log, indent=2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LA7lz21ODcCM",
        "outputId": "dda63359-b40b-4ced-def4-3197357ec6c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "741MB [02:15,  5.49MB/s]                         \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"source\": \"epc_domestic_register\",\n",
            "  \"ingest_ts\": \"2025-09-15T11:20:48.995612+00:00\",\n",
            "  \"url\": \"https://epc.opendatacommunities.org/api/v1/files/all-domestic-certificates.zip\",\n",
            "  \"s3_bucket\": \"uk-property-bronze\",\n",
            "  \"s3_key\": \"bronze/epc/domestic/epc-domestic-all-certificates.zip\",\n",
            "  \"size_bytes\": 6214054552\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fSVEEWczDdHs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}